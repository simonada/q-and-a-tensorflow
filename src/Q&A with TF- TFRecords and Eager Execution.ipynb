{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sdoneva/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sdoneva/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from itertools import chain\n",
    "from InputPreparator import EmbeddingsPreparator\n",
    "from InputPreparator import StoryParser\n",
    "import csv\n",
    "\n",
    "#to avoid a warning from TF 1.7 version see https://github.com/tensorflow/tensorflow/issues/18111\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_task_files(task_nr):\n",
    "    if task_nr==5:\n",
    "        return 'qa5_three-arg-relations_train.txt', \"qa5_three-arg-relations_test.txt\"\n",
    "    if task_nr==6:\n",
    "        return 'qa6_yes-no-questions_train.txt', 'qa6_yes-no-questions_test.txt'\n",
    "    if task_nr==10:\n",
    "        return 'qa10_indefinite-knowledge_train.txt', 'qa10_indefinite-knowledge_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_file = get_task_files(5)[0]\n",
    "test_set_file = get_task_files(5)[1]\n",
    "\n",
    "train_set_post_file = \"data/tasks_1-20_v1-2/en/\"+train_set_file\n",
    "test_set_post_file = \"data/tasks_1-20_v1-2/en/\"+test_set_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedder=EmbeddingsPreparator()\n",
    "story_parser=StoryParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_tokens = embedder.get_unique_tokens([train_set_post_file, test_set_post_file])\n",
    "word_to_index, index_to_embedding = embedder.load_embedding_from_disks(\"data/glove.6B.50d.txt\",vocab_tokens, with_indexes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_to_index) + 1, embed_dimensions))\n",
    "for word, i in word_to_index.items():\n",
    "    embedding_vector = index_to_embedding[i]\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_stories=story_parser.get_stories(train_set_post_file, False)\n",
    "test_stories=story_parser.get_stories(test_set_post_file, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store into a csv file: per row (context, question, answer)\n",
    "def store_to_csv(data, filename, vectors):\n",
    "    with open(filename,'w') as f:   \n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['context', 'question', 'answer'])\n",
    "        for story in data:\n",
    "            if vectors:\n",
    "                writer.writerow(story)\n",
    "            else:\n",
    "                temp=[]\n",
    "                context, question, answer= story\n",
    "                temp.append(' '.join(context))\n",
    "                temp.append(' '.join(question))\n",
    "                temp.append(''.join(answer))            \n",
    "                writer.writerow(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store_to_csv(train_stories, 'train_data.csv', False)\n",
    "store_to_csv(test_stories, 'test_data.csv', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(sentence, answer, word_to_index):\n",
    "    if not answer:\n",
    "        x=[]\n",
    "        for w in sentence.split():\n",
    "            w=w.replace(\"]\",\"\").replace(\"[\",\"\").replace(\"'\",\"\").replace(\",\",\"\")\n",
    "            w=w.lower().strip()\n",
    "            x.append(word_to_index[w]) \n",
    "        return x\n",
    "     \n",
    "    else:\n",
    "        # The Answer is one-hot encoded in our vocabulary matrix\n",
    "        y = np.zeros(len(word_to_index) + 1, dtype=int)\n",
    "        answ=sentence.lower().strip()\n",
    "        y[word_to_index[answ]] = 1\n",
    "        return y      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Convert via TFRecords\n",
    "- https://medium.com/@TalPerry/getting-text-into-tensorflow-with-the-dataset-api-ffb832c8bec6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sequence_to_tf_example(context, question, answer):\n",
    "        context_ids= vectorize(context, False, word_to_index)\n",
    "        question_ids= vectorize(question, False, word_to_index)\n",
    "        answer_ids= vectorize(answer, True, word_to_index)\n",
    "        ex = tf.train.SequenceExample()\n",
    "      \n",
    "        context_tokens = ex.feature_lists.feature_list[\"context\"]\n",
    "        question_tokens = ex.feature_lists.feature_list[\"question\"]\n",
    "        answer_tokens = ex.feature_lists.feature_list[\"answer\"]\n",
    "        \n",
    "        for token in context_ids:\n",
    "            context_tokens.feature.add().int64_list.value.append(token)\n",
    "        for token in question_ids:\n",
    "            question_tokens.feature.add().int64_list.value.append(token)\n",
    "        for token in answer_ids:\n",
    "            #print(token)\n",
    "            answer_tokens.feature.add().int64_list.value.append(token)\n",
    "\n",
    "        return ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_from_tfrecord(ex):\n",
    "    '''\n",
    "    Explain to TF how to go from a serialized example back to tensors\n",
    "    '''\n",
    "    sequence_features = {\n",
    "        \"context\": tf.FixedLenSequenceFeature([], dtype=tf.int64),\n",
    "        \"question\": tf.FixedLenSequenceFeature([], dtype=tf.int64),\n",
    "        \"answer\": tf.FixedLenSequenceFeature([], dtype=tf.int64)\n",
    "    }\n",
    "\n",
    "    # Parse the example (returns a dictionary of tensors)\n",
    "    _, sequence_parsed = tf.parse_single_sequence_example(\n",
    "        serialized=ex,\n",
    "        sequence_features=sequence_features\n",
    "    )\n",
    "\n",
    "    return {\"context\": sequence_parsed['context'], \"question\": sequence_parsed['question'],\n",
    "            \"answer\": sequence_parsed['answer']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_example_to_tfrecord(context, question, answer, tfrecord_file, writer):\n",
    "    example= sequence_to_tf_example(context, question, answer)\n",
    "    writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_data_to_tf_record(filename):\n",
    "    file_csv= filename+'.csv'\n",
    "    file_tfrecords= filename+'.tfrecords'\n",
    "    with open(file_csv) as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "        next(readCSV) #skip header\n",
    "        writer= tf.python_io.TFRecordWriter(file_tfrecords)\n",
    "        for row in readCSV:\n",
    "        #print(row[0], row[1], row[2])\n",
    "            write_example_to_tfrecord(row[0], row[1], row[2], file_tfrecords, writer)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_data_to_tf_record('train_data')\n",
    "write_data_to_tf_record('test_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dataset(path, batch_size=128):\n",
    "    '''\n",
    "    Makes  a Tensorflow dataset that is shuffled, batched and parsed.\n",
    "    '''\n",
    "    # Read a tf record file. This makes a dataset of raw TFRecords\n",
    "    dataset = tf.data.TFRecordDataset([path])\n",
    "    # Apply/map the parse function to every record. Now the dataset is a bunch of dictionaries of Tensors\n",
    "    dataset =  dataset.map(read_from_tfrecord)\n",
    "    #Shuffle the dataset\n",
    "    dataset = dataset.shuffle(buffer_size=10000)\n",
    "   \n",
    "    # specify padding for each tensor seperatly\n",
    "    dataset = dataset.padded_batch(batch_size, padded_shapes={\n",
    "        \"context\": tf.TensorShape([None]), \n",
    "        \"question\": tf.TensorShape([None]), \n",
    "        \"answer\": tf.TensorShape([None]) \n",
    "    })\n",
    "   \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_tfrecords= make_dataset('train_data.tfrecords')\n",
    "test_data_tfrecords= make_dataset('test_data.tfrecords', 1000) #no need to batch for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eager Execution Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate= 0.001\n",
    "vocab_size= len(index_to_embedding)\n",
    "embed_dimensions= 50\n",
    "num_units_gru= 50\n",
    "keep_prob= 0.5\n",
    "num_epochs= 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.embed = tf.keras.layers.Embedding(len(word_to_index) + 1,\n",
    "                            embed_dimensions,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)\n",
    "        self.grucell=tf.keras.layers.GRUCell(num_units_gru)\n",
    "        self.rnn=tf.keras.layers.RNN(self.grucell)\n",
    "        self.dense=tf.keras.layers.Dense(vocab_size, activation=tf.nn.softmax)\n",
    "        self.dropout=tf.keras.layers.Dropout(keep_prob)\n",
    "        \n",
    "    def predict(self, sentence, question):\n",
    "        encoded_sentence=self.embed(sentence)\n",
    "        encoded_sentence=self.rnn(encoded_sentence)\n",
    "        encoded_sentence=self.dropout(encoded_sentence)\n",
    "        \n",
    "        encoded_question=self.embed(question)\n",
    "        encoded_question=self.rnn(encoded_question)\n",
    "        encoded_question=self.dropout(encoded_question)\n",
    "        \n",
    "        merged= tf.keras.layers.concatenate([encoded_sentence, encoded_question])\n",
    "        pred= self.dense(merged)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(model, sent, quest, y):\n",
    "    prediction = model.predict(sent, quest)\n",
    "    return tf.keras.losses.categorical_crossentropy(y, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad(model, sent, quest, targets):\n",
    "    with tfe.GradientTape() as tape:\n",
    "        loss_value = loss(model, sent, quest, targets)\n",
    "        tf.contrib.summary.scalar(\"loss\", loss_value)\n",
    "    return tape.gradient(loss_value, model.variables), loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#global_step = tf.train.get_or_create_global_step() \n",
    "#summary_writer = tf.contrib.summary.create_file_writer('log/eager', flush_millis=10000) \n",
    "#with summary_writer.as_default():\n",
    "#    tf.contrib.summary.always_record_summaries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Loss at epoch 0: 2.9854490756988525\n",
      "Loss at epoch 10: 1.8376080989837646\n",
      "Loss at epoch 20: 1.3167572021484375\n",
      "Loss at epoch 30: 1.275816798210144\n",
      "Loss at epoch 40: 1.2570112943649292\n",
      "Loss at epoch 50: 1.268471360206604\n",
      "Loss at epoch 60: 1.1982849836349487\n",
      "Loss at epoch 70: 1.152419924736023\n",
      "Loss at epoch 80: 1.145656704902649\n",
      "Loss at epoch 90: 1.0608627796173096\n",
      "Loss at epoch 100: 1.0850512981414795\n",
      "Loss at epoch 110: 1.150460958480835\n",
      "Loss at epoch 120: 0.9572643637657166\n",
      "Loss at epoch 130: 0.9193277359008789\n",
      "Loss at epoch 140: 0.9598548412322998\n",
      "Loss at epoch 150: 0.8275853395462036\n",
      "Loss at epoch 160: 0.7587436437606812\n",
      "Loss at epoch 170: 0.6594508290290833\n",
      "Loss at epoch 180: 0.6024786233901978\n",
      "Loss at epoch 190: 0.6715546250343323\n",
      "\n",
      "Training time: \n",
      "17057.656500339508\n"
     ]
    }
   ],
   "source": [
    "model=Model()\n",
    "\n",
    "print('Training...')\n",
    "start_time = time.time()\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    start_get_batch = time.time()\n",
    "    for batch in tfe.Iterator(train_data_tfrecords): # 8 batches\n",
    "        elapsed_time_batch = time.time() - start_get_batch\n",
    "        print('get batch time', elapsed_time_batch )\n",
    "\n",
    "        answer = tf.keras.backend.cast(batch['answer'], 'float32')\n",
    "\n",
    "        grads, loss_value = grad(model, batch['context'], batch['question'], answer)\n",
    "        optimizer.apply_gradients(zip(grads, model.variables),\n",
    "                            global_step=tf.train.get_or_create_global_step())\n",
    "        \n",
    "    if i % 10 == 0:\n",
    "        print(\"Loss at epoch {}: {}\".format(i, np.mean(loss_value)))\n",
    "\n",
    "elapsed_time = time.time() - start_time        \n",
    "print()\n",
    "print('Training time: ')\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Testing Accuracy:\n",
      "0.68\n"
     ]
    }
   ],
   "source": [
    "#acc= tfe.metrics.Accuracy()\n",
    "\n",
    "for batch in tfe.Iterator(test_data_tfrecords): # 1 batch\n",
    "        answer = tf.keras.backend.cast(batch['answer'], 'float32')\n",
    "        prediction= model.predict(batch['context'], batch['question']) \n",
    "        pred=tf.cast(tf.argmax(prediction, 1), 'int32')\n",
    "        answ= tf.cast(tf.argmax(answer, 1), 'int32')\n",
    "        \n",
    "        corrects = tf.equal(pred, answ, 'int32')\n",
    "        accuracy = np.mean(tf.cast(corrects, tf.float32))\n",
    "        \n",
    "        print('Final Testing Accuracy:')\n",
    "        print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "get batch time 0.14572811126708984\n",
      "get grads time 6.74985408782959\n",
      "apply grads time 0.05928802490234375\n",
      "get batch time 6.957172155380249\n",
      "get grads time 7.01105523109436\n",
      "apply grads time 0.0019221305847167969\n",
      "get batch time 13.971900939941406\n",
      "get grads time 6.038722991943359\n",
      "apply grads time 0.0017819404602050781\n",
      "get batch time 20.01413893699646\n",
      "get grads time 5.174599885940552\n",
      "apply grads time 0.0018627643585205078\n",
      "get batch time 25.19239902496338\n",
      "get grads time 3.059516191482544\n",
      "apply grads time 0.0015461444854736328\n",
      "get batch time 28.2556471824646\n",
      "get grads time 4.6573851108551025\n",
      "apply grads time 0.0020339488983154297\n",
      "get batch time 32.917110204696655\n",
      "get grads time 5.347007989883423\n",
      "apply grads time 0.0018668174743652344\n",
      "get batch time 38.26903986930847\n",
      "get grads time 5.072596073150635\n",
      "apply grads time 0.0022809505462646484\n",
      "Loss at epoch 0: 2.0402767658233643\n",
      "get batch time 0.13411498069763184\n",
      "get grads time 7.4962158203125\n",
      "apply grads time 0.001859903335571289\n",
      "get batch time 7.634987831115723\n",
      "get grads time 4.842497825622559\n",
      "apply grads time 0.0019488334655761719\n",
      "get batch time 12.481986999511719\n",
      "get grads time 2.527782917022705\n",
      "apply grads time 0.0015022754669189453\n",
      "get batch time 15.013146877288818\n",
      "get grads time 3.6431329250335693\n",
      "apply grads time 0.0019230842590332031\n",
      "get batch time 18.65972399711609\n",
      "get grads time 5.139302968978882\n",
      "apply grads time 0.002274036407470703\n",
      "get batch time 23.803287029266357\n",
      "get grads time 5.869764804840088\n",
      "apply grads time 0.0017619132995605469\n",
      "get batch time 29.676345109939575\n",
      "get grads time 5.867703914642334\n",
      "apply grads time 0.001753091812133789\n",
      "get batch time 35.54980492591858\n",
      "get grads time 5.403876066207886\n",
      "apply grads time 0.0015511512756347656\n",
      "get batch time 0.17388296127319336\n",
      "get grads time 3.8746402263641357\n",
      "apply grads time 0.0014941692352294922\n",
      "get batch time 4.052162170410156\n",
      "get grads time 4.541001081466675\n",
      "apply grads time 0.0019838809967041016\n",
      "get batch time 8.5967538356781\n",
      "get grads time 5.484591960906982\n",
      "apply grads time 0.0018088817596435547\n",
      "get batch time 14.084795951843262\n",
      "get grads time 5.356698036193848\n",
      "apply grads time 0.0021250247955322266\n",
      "get batch time 19.445209980010986\n",
      "get grads time 3.169663906097412\n",
      "apply grads time 0.001589059829711914\n",
      "get batch time 22.618627786636353\n",
      "get grads time 2.4769980907440186\n",
      "apply grads time 0.0019512176513671875\n",
      "get batch time 25.09947896003723\n",
      "get grads time 5.9298601150512695\n",
      "apply grads time 0.0019102096557617188\n",
      "get batch time 31.033396005630493\n",
      "get grads time 5.422309875488281\n",
      "apply grads time 0.001531839370727539\n",
      "get batch time 0.1360020637512207\n",
      "get grads time 4.389351844787598\n",
      "apply grads time 0.003004789352416992\n",
      "get batch time 4.530313968658447\n",
      "get grads time 5.613877058029175\n",
      "apply grads time 0.0018382072448730469\n",
      "get batch time 10.148430109024048\n",
      "get grads time 4.264085054397583\n",
      "apply grads time 0.0017919540405273438\n",
      "get batch time 14.416093111038208\n",
      "get grads time 4.418693780899048\n",
      "apply grads time 0.0018739700317382812\n",
      "get batch time 18.83816695213318\n",
      "get grads time 3.810640811920166\n",
      "apply grads time 0.0015950202941894531\n",
      "get batch time 22.65243697166443\n",
      "get grads time 5.739763259887695\n",
      "apply grads time 0.0027980804443359375\n",
      "get batch time 28.397002935409546\n",
      "get grads time 5.132383823394775\n",
      "apply grads time 0.002290010452270508\n",
      "get batch time 33.53466606140137\n",
      "get grads time 6.346672296524048\n",
      "apply grads time 0.0025839805603027344\n",
      "get batch time 0.1787261962890625\n",
      "get grads time 3.427377223968506\n",
      "apply grads time 0.0014510154724121094\n",
      "get batch time 3.609804153442383\n",
      "get grads time 3.9016530513763428\n",
      "apply grads time 0.0017940998077392578\n",
      "get batch time 7.515648126602173\n",
      "get grads time 4.501661062240601\n",
      "apply grads time 0.0016818046569824219\n",
      "get batch time 12.020979404449463\n",
      "get grads time 5.390013933181763\n",
      "apply grads time 0.0018429756164550781\n",
      "get batch time 17.41497826576233\n",
      "get grads time 4.232335090637207\n",
      "apply grads time 0.0017960071563720703\n",
      "get batch time 21.651102304458618\n",
      "get grads time 5.271999835968018\n",
      "apply grads time 0.0014677047729492188\n",
      "get batch time 26.926554203033447\n",
      "get grads time 3.5001847743988037\n",
      "apply grads time 0.0017960071563720703\n",
      "get batch time 30.43090510368347\n",
      "get grads time 4.906725883483887\n",
      "apply grads time 0.0019328594207763672\n",
      "get batch time 0.12247300148010254\n",
      "get grads time 5.057629823684692\n",
      "apply grads time 0.001931905746459961\n",
      "get batch time 5.183911085128784\n",
      "get grads time 4.22296404838562\n",
      "apply grads time 0.0017809867858886719\n",
      "get batch time 9.410577058792114\n",
      "get grads time 4.231616973876953\n",
      "apply grads time 0.0022079944610595703\n",
      "get batch time 13.646324157714844\n",
      "get grads time 5.209168910980225\n",
      "apply grads time 0.0017778873443603516\n",
      "get batch time 18.85898208618164\n",
      "get grads time 3.328101873397827\n",
      "apply grads time 0.002805948257446289\n",
      "get batch time 22.191882133483887\n",
      "get grads time 6.067224025726318\n",
      "apply grads time 0.0017709732055664062\n",
      "get batch time 28.262272119522095\n",
      "get grads time 3.5259907245635986\n",
      "apply grads time 0.0024750232696533203\n",
      "get batch time 31.793418169021606\n",
      "get grads time 5.225957155227661\n",
      "apply grads time 0.0014851093292236328\n",
      "get batch time 0.11908817291259766\n",
      "get grads time 3.2460317611694336\n",
      "apply grads time 0.0015819072723388672\n",
      "get batch time 3.368947982788086\n"
     ]
    }
   ],
   "source": [
    "model=Model()\n",
    "\n",
    "print('Training...')\n",
    "start_time = time.time()\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    start_get_batch = time.time()\n",
    "    for batch in tfe.Iterator(train_data_tfrecords): # 8 batches\n",
    "        elapsed_time_batch = time.time() - start_get_batch\n",
    "        print('get batch time', elapsed_time_batch )\n",
    "\n",
    "        answer = tf.keras.backend.cast(batch['answer'], 'float32')\n",
    "\n",
    "        start_grad = time.time()\n",
    "        grads, loss_value = grad(model, batch['context'], batch['question'], answer)\n",
    "        elapsed_time_grads = time.time() - start_grad\n",
    "        print('get grads time', elapsed_time_grads)\n",
    "        \n",
    "        start_optim = time.time()\n",
    "        optimizer.apply_gradients(zip(grads, model.variables),\n",
    "                            global_step=tf.train.get_or_create_global_step())\n",
    "        elapsed_time_optim = time.time() - start_optim\n",
    "        print('apply grads time', elapsed_time_optim)\n",
    "        start_get_batch = time.time()\n",
    "        \n",
    "    if i % 10 == 0:\n",
    "        print(\"Loss at epoch {}: {}\".format(i, np.mean(loss_value)))\n",
    "\n",
    "elapsed_time = time.time() - start_time        \n",
    "print()\n",
    "print('Training time: ')\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
